{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dj_database_url\n",
      "  Downloading dj_database_url-0.5.0-py2.py3-none-any.whl (5.5 kB)\n",
      "Installing collected packages: dj-database-url\n",
      "Successfully installed dj-database-url-0.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dj_database_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.8.6-cp38-cp38-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.8.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dedupe\n",
      "  Downloading dedupe-2.0.6-cp38-cp38-manylinux1_x86_64.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 1.4 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting doublemetaphone\n",
      "  Downloading DoubleMetaphone-0.1-cp38-cp38-manylinux1_x86_64.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 3.2 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting highered>=0.2.0\n",
      "  Downloading highered-0.2.1-py2.py3-none-any.whl (3.3 kB)\n",
      "Collecting dedupe-hcluster\n",
      "  Downloading dedupe_hcluster-0.3.8-cp38-cp38-manylinux1_x86_64.whl (539 kB)\n",
      "\u001b[K     |████████████████████████████████| 539 kB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting categorical-distance>=1.9\n",
      "  Downloading categorical_distance-1.9-py3-none-any.whl (3.3 kB)\n",
      "Collecting affinegap>=1.3\n",
      "  Downloading affinegap-1.11-cp38-cp38-manylinux1_x86_64.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 4.6 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting simplecosine>=1.2\n",
      "  Downloading simplecosine-1.2-py2.py3-none-any.whl (3.2 kB)\n",
      "Collecting fastcluster\n",
      "  Downloading fastcluster-1.1.26-cp38-cp38-manylinux1_x86_64.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dedupe-variable-datetime\n",
      "  Downloading dedupe_variable_datetime-0.1.5-py3-none-any.whl (4.8 kB)\n",
      "Collecting haversine>=0.4.1\n",
      "  Downloading haversine-2.3.0-py2.py3-none-any.whl (5.5 kB)\n",
      "Collecting Levenshtein-search\n",
      "  Downloading Levenshtein_search-1.4.5-cp38-cp38-manylinux1_x86_64.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 7.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from dedupe) (3.7.4.3)\n",
      "Requirement already satisfied: numpy>=1.13 in /opt/conda/lib/python3.8/site-packages (from dedupe) (1.19.2)\n",
      "Collecting BTrees>=4.1.4\n",
      "  Downloading BTrees-4.7.2-cp38-cp38-manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting zope.index\n",
      "  Downloading zope.index-5.0.0-cp38-cp38-manylinux2010_x86_64.whl (102 kB)\n",
      "\u001b[K     |████████████████████████████████| 102 kB 5.9 MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting rlr>=2.4.3\n",
      "  Downloading rlr-2.4.5-py2.py3-none-any.whl (4.8 kB)\n",
      "Collecting pyhacrf-datamade>=0.2.0\n",
      "  Downloading pyhacrf_datamade-0.2.5-cp38-cp38-manylinux1_x86_64.whl (824 kB)\n",
      "\u001b[K     |████████████████████████████████| 824 kB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting datetime-distance\n",
      "  Downloading datetime_distance-0.1.3-py3-none-any.whl (4.1 kB)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.8/site-packages (from dedupe-variable-datetime->dedupe) (0.18.2)\n",
      "Collecting persistent>=4.1.0\n",
      "  Downloading persistent-4.6.4-cp38-cp38-manylinux2010_x86_64.whl (255 kB)\n",
      "\u001b[K     |████████████████████████████████| 255 kB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zope.interface in /opt/conda/lib/python3.8/site-packages (from BTrees>=4.1.4->dedupe) (5.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from zope.index->dedupe) (50.3.1.post20201107)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from zope.index->dedupe) (1.15.0)\n",
      "Collecting pylbfgs\n",
      "  Downloading PyLBFGS-0.2.0.13-cp38-cp38-manylinux1_x86_64.whl (212 kB)\n",
      "\u001b[K     |████████████████████████████████| 212 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.0 in /opt/conda/lib/python3.8/site-packages (from datetime-distance->dedupe-variable-datetime->dedupe) (2.8.1)\n",
      "Requirement already satisfied: cffi; platform_python_implementation == \"CPython\" in /opt/conda/lib/python3.8/site-packages (from persistent>=4.1.0->BTrees>=4.1.4->dedupe) (1.14.3)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi; platform_python_implementation == \"CPython\"->persistent>=4.1.0->BTrees>=4.1.4->dedupe) (2.20)\n",
      "Installing collected packages: doublemetaphone, pylbfgs, pyhacrf-datamade, highered, dedupe-hcluster, categorical-distance, affinegap, simplecosine, fastcluster, datetime-distance, dedupe-variable-datetime, haversine, Levenshtein-search, persistent, BTrees, zope.index, rlr, dedupe\n",
      "Successfully installed BTrees-4.7.2 Levenshtein-search-1.4.5 affinegap-1.11 categorical-distance-1.9 datetime-distance-0.1.3 dedupe-2.0.6 dedupe-hcluster-0.3.8 dedupe-variable-datetime-0.1.5 doublemetaphone-0.1 fastcluster-1.1.26 haversine-2.3.0 highered-0.2.1 persistent-4.6.4 pyhacrf-datamade-0.2.5 pylbfgs-0.2.0.13 rlr-2.4.5 simplecosine-1.2 zope.index-5.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import optparse\n",
    "import locale\n",
    "import itertools\n",
    "import io\n",
    "import csv\n",
    "\n",
    "import dj_database_url\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "\n",
    "import dedupe\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "register_adapter(numpy.int32, AsIs)\n",
    "register_adapter(numpy.int64, AsIs)\n",
    "register_adapter(numpy.float32, AsIs)\n",
    "register_adapter(numpy.float64, AsIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Readable(object):\n",
    "\n",
    "    def __init__(self, iterator):\n",
    "\n",
    "        self.output = io.StringIO()\n",
    "        self.writer = csv.writer(self.output)\n",
    "        self.iterator = iterator\n",
    "\n",
    "    def read(self, size):\n",
    "\n",
    "        self.writer.writerows(itertools.islice(self.iterator, size))\n",
    "\n",
    "        chunk = self.output.getvalue()\n",
    "        self.output.seek(0)\n",
    "        self.output.truncate(0)\n",
    "\n",
    "        return chunk\n",
    "\n",
    "\n",
    "def record_pairs(result_set):\n",
    "\n",
    "    for i, row in enumerate(result_set):\n",
    "        a_record_id, a_record, b_record_id, b_record = row\n",
    "        record_a = (a_record_id, a_record)\n",
    "        record_b = (b_record_id, b_record)\n",
    "\n",
    "        yield record_a, record_b\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "\n",
    "\n",
    "def cluster_ids(clustered_dupes):\n",
    "\n",
    "    for cluster, scores in clustered_dupes:\n",
    "        cluster_id = cluster[0]\n",
    "        for donor_id, score in zip(cluster, scores):\n",
    "            yield donor_id, cluster_id, score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if __name__ == '__main__':\n",
    "    # ## Logging\n",
    "\n",
    "    # Dedupe uses Python logging to show or suppress verbose output. Added\n",
    "    # for convenience.  To enable verbose output, run `python\n",
    "    # pgsql_big_dedupe_example.py -v`\n",
    "    optp = optparse.OptionParser()\n",
    "    optp.add_option('-v', '--verbose', dest='verbose', action='count',\n",
    "                    help='Increase verbosity (specify multiple times for more)'\n",
    "                    )\n",
    "    (opts, args) = optp.parse_args()\n",
    "    log_level = logging.WARNING\n",
    "    if opts.verbose:\n",
    "        if opts.verbose == 1:\n",
    "            log_level = logging.INFO\n",
    "        elif opts.verbose >= 2:\n",
    "            log_level = logging.DEBUG\n",
    "    logging.getLogger().setLevel(log_level)\n",
    "\n",
    "    # ## Setup\n",
    "    settings_file = 'pgsql_big_dedupe_example_settings'\n",
    "    training_file = 'pgsql_big_dedupe_example_training.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ## Setup\n",
    "    settings_file = 'pgsql_big_dedupe_example_settings'\n",
    "    training_file = 'pgsql_big_dedupe_example_training.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    start_time = time.time()\n",
    "    \n",
    "    read_con = psycopg2.connect(database=\"campaign-finance\",\n",
    "                        user=\"postgres\",\n",
    "                        password=\"\",\n",
    "                        host=\"172.16.238.13\",\n",
    "                        port=\"5432\",\n",
    "                        cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "\n",
    "    write_con = psycopg2.connect(database=\"campaign-finance\",\n",
    "                        user=\"postgres\",\n",
    "                        password=\"\",\n",
    "                        host=\"172.16.238.13\",\n",
    "                        port=\"5432\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    DONOR_SELECT = \"SELECT donor_id, city, name, zip, state, address \" \\\n",
    "                   \"from processed_donors\"\n",
    "\n",
    "    # ## Training\n",
    "\n",
    "    if os.path.exists(settings_file):\n",
    "        print('reading from ', settings_file)\n",
    "        with open(settings_file, 'rb') as sf:\n",
    "            deduper = dedupe.StaticDedupe(sf, num_cores=4)\n",
    "    else:\n",
    "\n",
    "        # Define the fields dedupe will pay attention to\n",
    "        #\n",
    "        # The address, city, and zip fields are often missing, so we'll\n",
    "        # tell dedupe that, and we'll learn a model that take that into\n",
    "        # account\n",
    "        fields = [{'field': 'name', 'type': 'String'},\n",
    "                  {'field': 'address', 'type': 'String', 'has missing': True},\n",
    "                  {'field': 'city', 'type': 'ShortString', 'has missing': True},\n",
    "                  {'field': 'state', 'type': 'ShortString', 'has missing': True},\n",
    "                  {'field': 'zip', 'type': 'ShortString', 'has missing': True},\n",
    "                  ]\n",
    "\n",
    "        # Create a new deduper object and pass our data model to it.\n",
    "        deduper = dedupe.Dedupe(fields, num_cores=4)\n",
    "\n",
    "        # Named cursor runs server side with psycopg2\n",
    "        with read_con.cursor('donor_select') as cur:\n",
    "            cur.execute(DONOR_SELECT)\n",
    "            temp_d = {i: row for i, row in enumerate(cur)}\n",
    "\n",
    "        # If we have training data saved from a previous run of dedupe,\n",
    "        # look for it an load it in.\n",
    "        #\n",
    "        # __Note:__ if you want to train from\n",
    "        # scratch, delete the training_file\n",
    "        if os.path.exists(training_file):\n",
    "            print('reading labeled examples from ', training_file)\n",
    "            with open(training_file) as tf:\n",
    "                deduper.prepare_training(temp_d, tf)\n",
    "        else:\n",
    "            deduper.prepare_training(temp_d)\n",
    "\n",
    "        del temp_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # ## Active learning\n",
    "\n",
    "        print('starting active labeling...')\n",
    "        # Starts the training loop. Dedupe will find the next pair of records\n",
    "        # it is least certain about and ask you to label them as duplicates\n",
    "        # or not.\n",
    "\n",
    "        # use 'y', 'n' and 'u' keys to flag duplicates\n",
    "        # press 'f' when you are finished\n",
    "        dedupe.console_label(deduper)\n",
    "        # When finished, save our labeled, training pairs to disk\n",
    "        with open(training_file, 'w') as tf:\n",
    "            deduper.write_training(tf)\n",
    "\n",
    "        # Notice our argument here\n",
    "        #\n",
    "        # `recall` is the proportion of true dupes pairs that the learned\n",
    "        # rules must cover. You may want to reduce this if your are making\n",
    "        # too many blocks and too many comparisons.\n",
    "        deduper.train(recall=0.90)\n",
    "\n",
    "        with open(settings_file, 'wb') as sf:\n",
    "            deduper.write_settings(sf)\n",
    "\n",
    "        # We can now remove some of the memory hogging objects we used\n",
    "        # for training\n",
    "        deduper.cleanup_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ## Blocking\n",
    "    print('blocking...')\n",
    "\n",
    "    # To run blocking on such a large set of data, we create a separate table\n",
    "    # that contains blocking keys and record ids\n",
    "    print('creating blocking_map database')\n",
    "    with write_con:\n",
    "        with write_con.cursor() as cur:\n",
    "            cur.execute(\"DROP TABLE IF EXISTS blocking_map\")\n",
    "            cur.execute(\"CREATE TABLE blocking_map \"\n",
    "                        \"(block_key text, donor_id INTEGER)\")\n",
    "\n",
    "    # If dedupe learned a Index Predicate, we have to take a pass\n",
    "    # through the data and create indices.\n",
    "    print('creating inverted index')\n",
    "\n",
    "    for field in deduper.fingerprinter.index_fields:\n",
    "        with read_con.cursor('field_values') as cur:\n",
    "            cur.execute(\"SELECT DISTINCT %s FROM processed_donors\" % field)\n",
    "            field_data = (row[field] for row in cur)\n",
    "            deduper.fingerprinter.index(field_data, field)\n",
    "\n",
    "    # Now we are ready to write our blocking map table by creating a\n",
    "    # generator that yields unique `(block_key, donor_id)` tuples.\n",
    "    print('writing blocking map')\n",
    "\n",
    "    with read_con.cursor('donor_select') as read_cur:\n",
    "        read_cur.execute(DONOR_SELECT)\n",
    "\n",
    "        full_data = ((row['donor_id'], row) for row in read_cur)\n",
    "        b_data = deduper.fingerprinter(full_data)\n",
    "\n",
    "        with write_con:\n",
    "            with write_con.cursor() as write_cur:\n",
    "                write_cur.copy_expert('COPY blocking_map FROM STDIN WITH CSV',\n",
    "                                      Readable(b_data),\n",
    "                                      size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # free up memory by removing indices\n",
    "    deduper.fingerprinter.reset_indices()\n",
    "\n",
    "    logging.info(\"indexing block_key\")\n",
    "    with write_con:\n",
    "        with write_con.cursor() as cur:\n",
    "            cur.execute(\"CREATE UNIQUE INDEX ON blocking_map \"\n",
    "                        \"(block_key text_pattern_ops, donor_id)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ## Clustering\n",
    "\n",
    "    with write_con:\n",
    "        with write_con.cursor() as cur:\n",
    "            cur.execute(\"DROP TABLE IF EXISTS entity_map\")\n",
    "\n",
    "            print('creating entity_map database')\n",
    "            cur.execute(\"CREATE TABLE entity_map \"\n",
    "                        \"(donor_id INTEGER, canon_id INTEGER, \"\n",
    "                        \" cluster_score FLOAT, PRIMARY KEY(donor_id))\")\n",
    "\n",
    "    with read_con.cursor('pairs', cursor_factory=psycopg2.extensions.cursor) as read_cur:\n",
    "        read_cur.execute(\"\"\"\n",
    "               select a.donor_id,\n",
    "                      row_to_json((select d from (select a.city,\n",
    "                                                         a.name,\n",
    "                                                         a.zip,\n",
    "                                                         a.state,\n",
    "                                                         a.address) d)),\n",
    "                      b.donor_id,\n",
    "                      row_to_json((select d from (select b.city,\n",
    "                                                         b.name,\n",
    "                                                         b.zip,\n",
    "                                                         b.state,\n",
    "                                                         b.address) d))\n",
    "               from (select DISTINCT l.donor_id as east, r.donor_id as west\n",
    "                     from blocking_map as l\n",
    "                     INNER JOIN blocking_map as r\n",
    "                     using (block_key)\n",
    "                     where l.donor_id < r.donor_id) ids\n",
    "               INNER JOIN processed_donors a on ids.east=a.donor_id\n",
    "               INNER JOIN processed_donors b on ids.west=b.donor_id\"\"\")\n",
    "\n",
    "        print('clustering...')\n",
    "        clustered_dupes = deduper.cluster(deduper.score(record_pairs(read_cur)),\n",
    "                                          threshold=0.5)\n",
    "\n",
    "        # ## Writing out results\n",
    "\n",
    "        # We now have a sequence of tuples of donor ids that dedupe believes\n",
    "        # all refer to the same entity. We write this out onto an entity map\n",
    "        # table\n",
    "\n",
    "        print('writing results')\n",
    "        with write_con:\n",
    "            with write_con.cursor() as write_cur:\n",
    "                write_cur.copy_expert('COPY entity_map FROM STDIN WITH CSV',\n",
    "                                      Readable(cluster_ids(clustered_dupes)),\n",
    "                                      size=10000)\n",
    "\n",
    "    with write_con:\n",
    "        with write_con.cursor() as cur:\n",
    "            cur.execute(\"CREATE INDEX head_index ON entity_map (canon_id)\")\n",
    "\n",
    "    # Print out the number of duplicates found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C.UTF-8'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " locale.setlocale(locale.LC_ALL, '')  # for pretty printing numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    with read_con.cursor() as cur:\n",
    "        cur.execute(\"DROP TABLE e_map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Donors (raw)\n",
      "         24556813.25: gmmb, inc.\n",
      "          14350000.0: good government coalition\n",
      "  12433621.810005147: aggregated individual contribution\n",
      "  11729787.940000001: onmessage, inc.\n",
      "  11379875.429999998: nc democratic leadership committee\n",
      "          11305000.0: citizens for a better north carolina\n",
      "          10222018.0: misty smithey\n",
      "           9755400.0: cooper for north carolina\n",
      "          8850440.35: nc democratic leadership committee\n",
      "           8850000.0: democratic action\n"
     ]
    }
   ],
   "source": [
    " with read_con.cursor() as cur:\n",
    "   \n",
    "        cur.execute(\n",
    "            \"SELECT CONCAT_WS(' ', donors.name) as name, \"\n",
    "            \"SUM(CAST(contributions.amount AS FLOAT)) AS totals \"\n",
    "            \"FROM donors INNER JOIN contributions \"\n",
    "            \"USING (donor_id) \"\n",
    "            \"GROUP BY (donor_id) \"\n",
    "            \"ORDER BY totals DESC \"\n",
    "            \"LIMIT 10\"\n",
    "        )\n",
    "\n",
    "        print(\"Top Donors (raw)\")\n",
    "        for row in cur:\n",
    "            row['totals'] = row['totals']\n",
    "            print('%(totals)20s: %(name)s' % row)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Donors (deduped)\n",
      "         27648935.78: nc democratic leadership building fund\n",
      "          25527066.7: gmmb inc\n",
      "  17567691.199999984: north carolina dec\n",
      "          14350000.0: good government coalition\n",
      "  12433621.810034692: aggregated individual contribution\n",
      "  11926236.380000003: onmessage\n",
      "          11305000.0: citizens for a better north carolina\n",
      "  11035671.030000003: buying time\n",
      "  10643647.379999965: nc democcratic party\n",
      "         10222098.25: misty smithey\n"
     ]
    }
   ],
   "source": [
    "    with read_con.cursor() as cur:\n",
    "               \n",
    "        cur.execute(\"CREATE TEMPORARY TABLE e_map \"\n",
    "                    \"AS SELECT COALESCE(canon_id, donor_id) AS canon_id, donor_id \"\n",
    "                    \"FROM entity_map \"\n",
    "                    \"RIGHT JOIN donors USING(donor_id)\")\n",
    "        \n",
    "        cur.execute(\n",
    "            \"SELECT donors.name AS name, \"\n",
    "            \"donation_totals.totals AS totals \"\n",
    "            \"FROM donors INNER JOIN \"\n",
    "            \"(SELECT contributions.canon_id, SUM(CAST(amount AS FLOAT)) AS totals \"\n",
    "            \" FROM contributions INNER JOIN e_map \"\n",
    "            \" USING (donor_id) \"\n",
    "            \" GROUP BY (contributions.canon_id) \"\n",
    "            \" ORDER BY totals \"\n",
    "            \" DESC LIMIT 10) \"\n",
    "            \"AS donation_totals ON donors.donor_id=donation_totals.canon_id \"\n",
    "            \"WHERE donors.donor_id = donation_totals.canon_id\"\n",
    "        )\n",
    "\n",
    "        print(\"Top Donors (deduped)\")\n",
    "        for row in cur:\n",
    "            row['totals'] = row['totals']\n",
    "            print('%(totals)20s: %(name)s' % row)\n",
    "            \n",
    "        cur.execute(\"SELECT * FROM e_map\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    read_con.close()\n",
    "    write_con.close()\n",
    "\n",
    "    print('ran in', time.time() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
